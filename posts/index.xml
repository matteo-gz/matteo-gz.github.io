<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>All Posts - matteo's blog</title><link>https://matteo-gz.github.io/posts/</link><description>All Posts | matteo's blog</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sat, 30 Dec 2023 17:01:21 +0800</lastBuildDate><atom:link href="https://matteo-gz.github.io/posts/" rel="self" type="application/rss+xml"/><item><title>小米解锁BL</title><link>https://matteo-gz.github.io/posts/xiaomiunlock/</link><pubDate>Sat, 30 Dec 2023 17:01:21 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/xiaomiunlock/</guid><description>这个办法是通过闲鱼APP上找人,利用UBL key解锁小米BL.
此方法需要消耗金钱,即购买UBL key的钱.
免费的方法为 https://github.com/MlgmXyysd/Xiaomi-HyperOS-BootLoader-Bypass ,但是无法解锁小米14之后的机型.
准备工作 下载客户端 https://xiaomikey.net/ window环境需要
下载驱动 https://xiaomiflashtool.com/
让xiaomikey客户端识别出小米手机,
理论上xiaomikey软件里面包含了xiaomiflash
下载ADB https://developer.android.com/tools/releases/platform-tools
将路径追加尽系统path.
adb devices 准备刷机 小米手机进入开发者模式 点击系统版本 连续点击5次 即可进入开发者模式
开发者模式下进行USB调试 要求
必须登录小米账号 必须插入sim卡 必须使用移动网络 在开发者模式下 打开允许USB调试.
电脑插入手机,手机选择信任电脑的连接.
adb进入fastboot模式 # 查看设备 adb devices # 进入fastboot adb reboot bootloader 登录xiaomikey 打开软件,选择 unlock boot- ubl key.
点击 unlock bootloader
此时会读取fastboot模式下的小米手机信息.
此时会生成 UBL token.
将UBL token 兑换成 UBL key.
然后填入UBL key 点击apply ubl key 可解锁.
过程中确保网络能连接小米服务器.
若操作失败 如果点击unlock bootloader后遇到duplicate token的话</description></item><item><title>socks5 协议</title><link>https://matteo-gz.github.io/posts/socks5/</link><pubDate>Wed, 13 Dec 2023 21:16:44 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/socks5/</guid><description>资料 rfc1928 SOCKS Protocol Version 5
https://datatracker.ietf.org/doc/html/rfc1928
socks5目标: TCP 和 UDP 域中的客户端-服务器应用程序提供一个框架，以便方便、安全地使用网络防火墙的服务
字节序 0x01234567 在字节顺序中表现
Big-Endian
0x100: 0x01 0x101: 0x23 0x102: 0x45 0x103: 0x67 Little-Endian
0x100: 0x67 0x101: 0x45 0x102: 0x23 0x103: 0x01 rfc1929 Username/Password Authentication for SOCKS V5
https://datatracker.ietf.org/doc/html/rfc1929
过程 1.TCP连接 3次握手后,客户端 连接 服务端socks5端口1080
2.协商环节 2.1.客户端发送给服务端 X'05&amp;rsquo; , X开头的数据都是十六进制
报文格式:
+----+----------+----------+ |VER | NMETHODS | METHODS | +----+----------+----------+ | 1 | 1 | 1 to 255 | +----+----------+----------+ 字段解释:</description></item><item><title>显卡相关</title><link>https://matteo-gz.github.io/posts/gpu/</link><pubDate>Sat, 12 Aug 2023 11:47:27 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/gpu/</guid><description>显卡挑选 原文which gpu for deep learning by Tim Dettmers - 2023/01/30
对此摘抄了 显卡工作原理 与 显卡关键因素
算力产品 AutoDL
显卡工作原理 结论 许多人说GPU快是因为 matrix multiplication 矩阵乘法 和 convolution 卷积 高效,Tim Dettmers认为是memory bandwidth 内存带宽.
内存带宽优点 CPU是延迟优化的,而GPU是带宽优化的.类比CPU是法拉利,GPU是大卡车, 在从 随机 A点 运货到 随机 B点 过程中,
CPU可以快速获取 RAM Random Access Memory 中的一些memory,而GPU则更慢(latency 延迟 要高得多), 但是CPU需要多次来回才能完成其工作,而GPU可以一次获取更多memory 内存.
CPU擅长快速获取少量内存(5 3 7), 而GPU擅长获取大量内存(矩阵乘法:(A*B)*C).
最好的CPU有大约 50GB/s,而最好的GPU有 750GB/s 的内存带宽.
如果计算操作需要的内存越多,GPU相对于CPU的优势就越明显.但是仍然存在可能会损害GPU性能的延迟.
一辆大卡车每次旅行可能都能拿起很多包裹,但问题是你要等很长时间,直到下一组包裹到达. 如果不解决这个问题,即使对于大量数据,GPU 也会非常慢.那么这个问题是如何解决的呢?
线程并行 如果你要求一辆大卡车进行多次旅行来取包裹,一旦卡车离开进行下一次旅行,你总是会等待很长时间才能收到下一批包裹——卡车只是很慢.
但是,如果您现在使用由法拉利和大型卡车组成的车队(线程并行),并且你有很多包裹(大块内存如 matrix 矩阵)的工作,那么你将等待第一辆卡车,
但在那之后你将根本没有等待时间 — 卸载包裹需要花费大量时间,以至于所有卡车都将在卸载位置B排队,以便你始终可以直接访问你的包裹(memory). 这有效地隐藏了延迟,以便GPU提供高带宽,同时在线程并行性下隐藏其延迟 — 因此对于大块内存,GPU提供最佳的内存带宽, 同时几乎没有由于线程并行延迟而导致的缺点.</description></item><item><title>Window偏好优化</title><link>https://matteo-gz.github.io/posts/win/</link><pubDate>Fri, 11 Aug 2023 15:19:51 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/win/</guid><description><![CDATA[杂项 默认展开win11的右键收缩菜单 How to Get Back Classic (Full) Context Menu in Windows 11
reg add HKCU\Software\Classes\CLSID\{86ca1aa0-34aa-4e8b-a509-50c905bae2a2}\InprocServer32 /ve /d &#34;&#34; /f window图标快捷箭头优化 How to remove arrows from shortcuts in Windows 11
REG ADD &#34;HKEY_LOCAL_MACHINE\SOFTWARE\Microsoft\Windows\CurrentVersion\Explorer\Shell Icons&#34; /v 29 /t REG_SZ /d &#34;&#34; /f 分辨率 参考来源 【硬件科普】换了更高分辨率的显示器，软件界面反而模糊-Windows缩放详解
针对27寸 2K屏幕
DSR调整 Nvidia control panel &gt;&gt; 3D settings &gt;&gt; Manage 3D settings &gt;&gt; global settings &gt;&gt; DSR-Factors 选择2.25x DL
显示设置调整 display resolution 选择 3840x2160
Scale 选择 250%]]></description></item><item><title>llama2模型 类chatGPT产品体验</title><link>https://matteo-gz.github.io/posts/llama/</link><pubDate>Wed, 09 Aug 2023 22:27:59 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/llama/</guid><description>简介 一个语言模型,类似chatGPT. 可以本地部署, 下面列下规格
7b 需要6～8GB显存 13b 需要13～16GB显存 70b 需要48GB显存,可以2张4090 安装 申请和下载模型 点击 https://huggingface.co/meta-llama
选择模型 Llama2-chat,13B 下载
将会跳转到Facebook公司的申请界面,勾选协议,提交申请
平替方案 如果觉得麻烦
下载这个GGML模型 https://huggingface.co/TheBloke/Firefly-Llama2-13B-v1.2-GGML
firefly-llama2-13b-v1.2.ggmlv3.q6_K.bin
文件链接 https://huggingface.co/TheBloke/Firefly-Llama2-13B-v1.2-GGML/blob/main/firefly-llama2-13b-v1.2.ggmlv3.q6_K.bin
python虚拟环境 # 创建python虚拟环境 python -m venv ./venv # 进行python虚拟环境 ./venv/Script/activate 安装pytorch with cuda # https://pytorch.org/get-started/locally/ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 安装ui # 安装ui git clone https://github.com/oobabooga/text-generation-webui.git # 进入ui目录 cd text-generation-webui # 安装ui依赖 pip install -r requirements.txt # 修复bitsandbytes bug pip uninstall bitsandbytes pip install bitsandbytes-windows # 启动UI python server.</description></item><item><title>音频转文字 Whisper</title><link>https://matteo-gz.github.io/posts/whisper/</link><pubDate>Tue, 08 Aug 2023 19:31:19 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/whisper/</guid><description><![CDATA[简介 一个音频转文本的模型 https://github.com/openai/whisper
依赖 python ffmpeg 命令行使用 验证GPU 在venv环境下 输入whisper -h
&ndash;device # 如果默认是cpu 则证明cuda安装失败
# 这样也能验证cuda是否安装了 python -c &#34;import torch; print(torch.version.cuda)&#34; 重装torch gpu版本
# 卸载纯cpu版 pip uninstall torch # clean cache pip cache purge # copy from https://pytorch.org/get-started/locally/ pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 再次验证 whisper -h
&ndash;device # 如果默认是cuda 则证明成功
运行demo
whisper {vocieFile} --model large-v2 --model_dir {modelPath} \ --ourput_dir {outputDir} --output_format txt --device cuda --language Chinese 界面版 界面版 https://github.]]></description></item><item><title>咒语生成图片 Stable Diffusion web UI</title><link>https://matteo-gz.github.io/posts/stablediffusionwebui/</link><pubDate>Tue, 08 Aug 2023 11:20:56 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/stablediffusionwebui/</guid><description>简介 旨在个人快速回忆Stable Diffusion相关关系
Stable Diffusion 是一个深度学习,文转图模型.
Stable Diffusion web UI 即面向用户提供界面操作工具, 链接为
https://github.com/AUTOMATIC1111/stable-diffusion-webui
其中该界面为英语,支持汉化插件
https://github.com/VinsonLaro/stable-diffusion-webui-chinese
PyTorch 官网 https://pytorch.org/ 是一个深度学习的框架
我的安装选择
OS: Windows package: pip language: python compute Platform: CUDA 教程 https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki
启动 下载完web UI后,运行webui-user.bat即可
在运行过程避免不了报错,很考验python debug能力。
初始化
设置webui-user.bat参数
set PYTHON= #python文件路径 set GIT= # git文件路径 set VENV_DIR= # venv目录 set COMMANDLINE_ARGS=--xformers # 启动参数 将python,git的bin path 加入系统环境变量下的path。
另外确保安装了Microsoft Visual C++ Redistributable vc_redist.x64.exe
安装过程需要网络稳定,需要下载很多pkg
缓存冲突解决
直接删除./venv/lib/site-packages/下冲突的包,重新运行脚本.
或者命令行搞定
# 卸载具体包 pip uninstall 具体包 # clean cache pip cache purge 模型位置</description></item><item><title>关于Go代码生成</title><link>https://matteo-gz.github.io/posts/gencode/</link><pubDate>Mon, 07 Aug 2023 16:24:58 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/gencode/</guid><description>目的 Go Kitex提供了自定义模板生成代码,使用起来挺方便的.
有望改造成支持公司内部go代码生成工具
https://www.cloudwego.io/zh/docs/kitex/tutorials/code-gen/custom_tpl/ 处理要素 cli工具整合 这里推荐2个包,封装了cli和参数解析
https://github.com/spf13/cobra https://github.com/spf13/pflag 基本数据对象定义 提供了文件生成时的数据渲染对象 PackageInfo
模板渲染使用html/template语法
额外参数在context中解析 cli传递参数过多会带来复杂性,需要额外的.json文件数据提供在上下文中解析.
多模板文件定义 通过指定目录加载其下所有模板文件
模板文件格式为.yaml
生成的文件路径也支持变量解析.
其中能独立定义新增代码部分
统一入口 初始化和更新行为都为同一命令,减少处理成本
额外点 多任务并行处理文件生成 跨平台支持</description></item><item><title>go pkg包设计</title><link>https://matteo-gz.github.io/posts/component/</link><pubDate>Mon, 07 Aug 2023 15:28:13 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/component/</guid><description><![CDATA[设计考虑 以下是个人的建议,随便看看了解下
context支持 一般入参数context是第一个参数,提供了timeout机制
logger接口定义 在组件的初始化时,将使用方的日志实例进行接口约束,
要求实现其debug,error级别method.
可以参考gorm的logger设计
hook注入 方便使用方对metric的收集,可以参考redis hook设计
布局了解 这里只是提供参考建议
go官方 https://github.com/golang-standards/project-layout 知道internal目录对内部系统的保护
链接 https://go.dev/doc/go1.4#internalpackages 跨平台支持 查看支持平台
go tool dist list 条件编译约束
//go:build (linux &amp;&amp; 386) || (darwin &amp;&amp; !cgo) https://pkg.go.dev/cmd/go#hdr-Build_constraints
linters集成 https://golangci-lint.run/ 提高代码质量
版本约束 具体查看 https://go.dev/ref/mod#versions
依赖注入 https://github.com/google/wire
使得依赖关系变得整洁
参数配置 Functional Options Pattern,代码优雅
IDL https://protobuf.dev/ 利用protobuf提供接口和配置的定义.
验证项目得分 https://goreportcard.com/
评估项含
gofmt go vet go lint gocyclo ]]></description></item><item><title>聊天系统设计调研</title><link>https://matteo-gz.github.io/posts/im/</link><pubDate>Mon, 07 Aug 2023 12:23:04 +0800</pubDate><author>Author</author><guid>https://matteo-gz.github.io/posts/im/</guid><description>总体架构分层
接入层,无状态 逻辑层 存储层 这里是参考了陌陌设计
连接层 这里有2种方案选择
WebSocket MQTT 关于MQTT,有EMQX中间件.
消息上线可以走HTTP接口,即客户端发给服务端消息
消息下行可以走EMQX订阅主题,即服务端推送消息给客户端
连接层属于无状态,支持水平扩展.
逻辑层 通用消息格式定制
以及业务路由分发决策
设计规则
不丢消息 需要业务ACK机制
消息去重 服务端与客户端都需要做到
时序一致性 可以用snowflake算法.
利用局部性原理减少id生成器压力,有利于水平扩展
缓存设计 由于实时系统,基本上有一层cache,类似内核系统的buffer。
多级缓冲,面向c端场景的话一般链路不会太长,而且要考虑数据回种,分布式一致性挑战.
大概local cache -&amp;gt; redis （读写分离)-&amp;gt; database（读写分离)
其他考虑场景 接入APP推送 接入风控 数据多端同步 多条消息打包与压缩格式设计 万人大群的数据扇出 明星突发性空降某频道 数据埋点监控 系统自动扩容 存储层 离线消息 如果需求量不大,Mysql即可以解决.
聊天应用的读写比例大概是1:1.
数据库 Cassandra
Discord公司有在用.偏向AP场景
HBase
Facebook公司有在用.偏向CP场景
个人想法 前期用Mysql,但是封装一层接口,为后期数据库迁移作准备.
大数据库选择要还看云数据库商支持程度,说不定还有其他云产品合适.
对象存储 minio
消息队列 rabbitMQ比较适合业务,
但是追求高吞吐的kafka,自己必须封装多一层机制来保证业务的一致性.
后续有的话再更新</description></item></channel></rss>